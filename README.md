# Model Architecture search for Visual Question Answering (Stanford CS224N/CS230 Winter 2017-18)
Building off/to architecture from [Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge by Teney et al](https://arxiv.org/pdf/1708.02711.pdf).

![Model architecture](https://i.imgur.com/phBHIqZ.png)


## Thank You to: [Mark](https://github.com/markdtw)
- This Project uses Mark's work to preprocess the MSCOCO data 

## Prerequisites
- Python 2.7+
- [NumPy](http://www.numpy.org/)
- [PyTorch](http://pytorch.org/)
- [tqdm](https://pypi.python.org/pypi/tqdm) (visualizing preprocessing progress only)
- [nltk](http://www.nltk.org/install.html) (and [this](https://nlp.stanford.edu/software/tokenizer.shtml) to tokenize questions)


## Data
- [VQA 2.0](http://visualqa.org/download.html)
- [COCO 36 features pretrained resnet model](https://github.com/peteanderson80/bottom-up-attention#pretrained-features)
- [GloVe pretrained Wikipedia+Gigaword word embedding](https://nlp.stanford.edu/projects/glove/)
- Already Tokenized Questions and Answers!!!! (https://drive.google.com/drive/folders/0B5j6QKJb0ztbYmVXT0hBUF91RHM)


## Resources
- [The paper](https://arxiv.org/pdf/1708.02711.pdf).
- [Their CVPR Workshop slides](http://cs.adelaide.edu.au/~Damien/Research/VQA-Challenge-Slides-TeneyAnderson.pdf).

